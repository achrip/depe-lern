{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from tensorflow import keras as K\n",
    "from keras import Input\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Embedding, LSTM, TextVectorization\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "We are going to use a sentiment analysis dataset from huggingface called\n",
    "`winvoker/turkish-sentiment-analysis-dataset` (original link [here]()). The\n",
    "dataset is consists of ~500000 rows, 3 columns (`text`, `label`, and `dataset`)\n",
    "and split into a ratio of 90:10 for the train and test splits.\n",
    "\n",
    "Using the `datasets` library, we can import the dataset using `load_dataset()`.\n",
    "This enables us to import the whole dataset or just a certain split of the\n",
    "dataset itself (meaning either the training or testing split). We can also slice\n",
    "the split to get a certain amount or a certain range of indices from the split\n",
    "(more about slicing the dataset split [here]())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'> \n",
      " <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'dataset'],\n",
       "    num_rows: 2448\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_NAME = 'winvoker/turkish-sentiment-analysis-dataset'\n",
    "ds_train = load_dataset(DATASET_NAME,\n",
    "                  split='train[:5%]')\n",
    "ds_test = load_dataset(DATASET_NAME, \n",
    "                       split='test[:5%]')\n",
    "print(type(ds_train), '\\n', type(ds_test))\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of loading a slice of the dataset from the training split (5% of the training split, to be precise). As we can see, it is an `arrow_dataset.Dataset` type object. \n",
    "\n",
    "But if we import the whole dataset, we can see that it is infact a `dataset_dict.DatasetDict` object. This is because the `load_dataset()` imports each split as a dictionary index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'dataset'],\n",
       "        num_rows: 440679\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'dataset'],\n",
       "        num_rows: 48965\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(DATASET_NAME)\n",
    "print(type(ds))\n",
    "ds\n",
    "del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, we want to split the dataset into three (train, validation, and test). As such, we have to split the training dataset using a 90:10 ratio to create a validation set.\n",
    "Then create a new `DatasetDict` object based on the new 80:10:10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.remove_columns(column_names='dataset')\n",
    "ds_test = ds_test.remove_columns(column_names='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n",
       "    validation: <PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n",
       "    test: <PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_temp = ds_train.train_test_split(test_size=.1)\n",
    "\n",
    "tf_ds_train = ds_temp['train'].to_tf_dataset(columns='text', \n",
    "                                             label_cols='label', \n",
    "                                             batch_size=64, \n",
    "                                             shuffle=True)\n",
    "tf_ds_validation = ds_temp['test'].to_tf_dataset(columns='text', \n",
    "                                                label_cols='label', \n",
    "                                                batch_size=64, \n",
    "                                                shuffle=True)\n",
    "tf_ds_test = ds_test.to_tf_dataset(columns='text', \n",
    "                                    label_cols='label', \n",
    "                                    batch_size=64, \n",
    "                                    shuffle=True)\n",
    "\n",
    "ds_new = DatasetDict({\n",
    "    'train': tf_ds_train,\n",
    "    'validation': tf_ds_validation,\n",
    "    'test': tf_ds_test,\n",
    "})\n",
    "ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_sequence_length = 250\n",
    "\n",
    "vectorization_layer = TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation', \n",
    "    max_tokens=max_features, \n",
    "    output_mode='int', \n",
    "    output_sequence_length=max_sequence_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = ds_new['train'].map(lambda x, y: (x,y))\n",
    "# text_train\n",
    "vectorization_layer.adapt(text_train.map(lambda x, y: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PrefetchDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m vectorization_layer(\u001b[43mds_new\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m validation_ds \u001b[38;5;241m=\u001b[39m vectorization_layer(ds_new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m vectorization_layer(ds_new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PrefetchDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_ds = vectorization_layer(ds_new['train'])\n",
    "validation_ds = vectorization_layer(ds_new['validation'])\n",
    "test_ds = vectorization_layer(ds_new['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(None, ), dtype=\"string\"), \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Dense(3, activation=\"softmax\")\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
